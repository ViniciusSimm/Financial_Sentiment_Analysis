{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import tokenize\n",
    "import nltk\n",
    "from gensim.models import KeyedVectors\n",
    "from string import punctuation\n",
    "import unidecode\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames=['output','text']\n",
    "df = pd.read_csv('all-data.csv', names=colnames, header=None)\n",
    "all_sentences = [text for text in df.text]\n",
    "words = ' '.join(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "token_space = tokenize.WhitespaceTokenizer()\n",
    "token_punct = tokenize.WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to transform and clean list of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformSentence(list_of_sentences):\n",
    "    \n",
    "    sentences_after_stopwords = list()\n",
    "    \n",
    "    for sentence in list_of_sentences:\n",
    "        new_sentence = list()\n",
    "        words_from_sentence = token_space.tokenize(sentence)\n",
    "        for word in words_from_sentence:\n",
    "            if word not in stop_words:\n",
    "                new_sentence.append(word)\n",
    "        sentences_after_stopwords.append(\" \".join(new_sentence))\n",
    "\n",
    "    sentences_after_stopwords_puncts = list()\n",
    "\n",
    "    for sentence in sentences_after_stopwords:\n",
    "        for punct_to_change in punctuation:\n",
    "            sentence = sentence.replace(punct_to_change,\"\")\n",
    "        sentences_after_stopwords_puncts.append(sentence)\n",
    "\n",
    "    sentences_after_stopwords_puncts_lower = list()\n",
    "\n",
    "    for sentence in sentences_after_stopwords_puncts:\n",
    "        sentence = sentence.lower()\n",
    "        sentences_after_stopwords_puncts_lower.append(sentence)\n",
    "\n",
    "    stop_words_no_accent = list()\n",
    "\n",
    "    for word in stop_words:\n",
    "        for punct_to_change in punctuation:\n",
    "            word = word.replace(punct_to_change,\"\")\n",
    "        stop_words_no_accent.append(word)\n",
    "    \n",
    "    sentences_after_stopwords_puncts_lower_stopwords = list()\n",
    "\n",
    "    for sentence in sentences_after_stopwords_puncts_lower:\n",
    "        new_sentence = list()\n",
    "        words_from_sentence = token_space.tokenize(sentence)\n",
    "        for word in words_from_sentence:\n",
    "            if word not in stop_words_no_accent:\n",
    "                new_sentence.append(word)\n",
    "        sentences_after_stopwords_puncts_lower_stopwords.append(\" \".join(new_sentence))\n",
    "\n",
    "    sentences_after_stopwords_puncts_lower_stopwords_number = list()\n",
    "\n",
    "    for sentence in sentences_after_stopwords_puncts_lower_stopwords:\n",
    "        new_sentence = list()\n",
    "        words_from_sentence = token_space.tokenize(sentence)\n",
    "        for word in words_from_sentence:\n",
    "            if not word.isnumeric():\n",
    "                new_sentence.append(word)\n",
    "            else:\n",
    "                new_sentence.append(\"0\")\n",
    "        sentences_after_stopwords_puncts_lower_stopwords_number.append(\" \".join(new_sentence))\n",
    "\n",
    "    return sentences_after_stopwords_puncts_lower_stopwords_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_sentences = transformSentence(list(df.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the imbalance between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4846, 600)\n",
      "(4846,)\n",
      "neutral     2879\n",
      "positive    1363\n",
      "negative     604\n",
      "Name: output, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=False,max_features=600)\n",
    "vector_tfidf = tfidf.fit_transform(treated_sentences)\n",
    "print(vector_tfidf.shape)\n",
    "print(df.output.shape)\n",
    "print(df.output.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8637, 600)\n",
      "(8637,)\n",
      "neutral     2879\n",
      "positive    2879\n",
      "negative    2879\n",
      "Name: output, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "smote = SMOTE(random_state=100)\n",
    "X_resampled, Y_resampled = smote.fit_resample(vector_tfidf,df.output)\n",
    "print(X_resampled.shape)\n",
    "print(Y_resampled.shape)\n",
    "print(Y_resampled.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting in Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size=0.1\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_resampled,Y_resampled,random_state = 100,test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (7773, 600)\n",
      "test: (864, 600)\n"
     ]
    }
   ],
   "source": [
    "print(\"train:\",X_train.shape)\n",
    "print(\"test:\",X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7557870370370371"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression(solver = 'lbfgs',penalty = 'l2',max_iter = 5000)\n",
    "logistic_regression.fit(X_train,Y_train)\n",
    "logistic_regression.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7800925925925926"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression(solver = 'lbfgs',penalty = 'none',max_iter = 5000)\n",
    "logistic_regression.fit(X_train,Y_train)\n",
    "logistic_regression.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7557870370370371"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression(solver = 'sag',penalty = 'l2',max_iter = 5000)\n",
    "logistic_regression.fit(X_train,Y_train)\n",
    "logistic_regression.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7777777777777778"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression(solver = 'sag',penalty = 'none',max_iter = 5000)\n",
    "logistic_regression.fit(X_train,Y_train)\n",
    "logistic_regression.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6759259259259259"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes = MultinomialNB(alpha=1)\n",
    "naive_bayes.fit(X_train,Y_train)\n",
    "naive_bayes.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8587962962962963"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomForest = RandomForestClassifier(random_state= 100)\n",
    "randomForest.fit(X_train,Y_train)\n",
    "randomForest.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": np.arange(10, 500, step=10),\n",
    "    \"criterion\": ['gini', 'entropy'],\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    \"max_depth\": np.arange(2, 10, step=1),\n",
    "    \"min_samples_split\": np.arange(2, 10, step=2),\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"bootstrap\": [True,False],\n",
    "}\n",
    "randomForest = RandomForestClassifier(random_state=100)\n",
    "random_cv = RandomizedSearchCV(randomForest, param_grid, n_iter=80, cv=5, n_jobs=-1, random_state = 100)\n",
    "rcv = random_cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 260,\n",
       " 'min_samples_split': 4,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 9,\n",
       " 'criterion': 'gini',\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65625"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomForest = RandomForestClassifier(**rcv.best_params_, random_state = 100)\n",
    "randomForest.fit(X_train,Y_train)\n",
    "randomForest.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "051f523ea69bc1770ecd2306c10409abac68aa2062faba780e671356775dd235"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
